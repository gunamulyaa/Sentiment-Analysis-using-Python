# -*- coding: utf-8 -*-
"""Naivebayes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_wrg2serfuB1PVXjFejxBlf0TegHYBmM

**Menambahkan File Crawling Data**
"""

import pandas as pd

# Membaca file CSV pertama dan mengambil kolom yang diperlukan
df1 = pd.read_csv('/content/sample_data/crawling1.csv', usecols=['full_text', 'username'])

# Membaca file CSV kedua dan mengambil kolom yang diperlukan
df2 = pd.read_csv('/content/sample_data/crawling2.csv', usecols=['full_text', 'username'])

# Menggabungkan kedua dataframe secara vertikal
combined_df = pd.concat([df1, df2], ignore_index=True)

# Menyimpan dataframe yang sudah digabungkan ke file CSV baru (opsional)
combined_df.to_csv('combined_dataset.csv', index=False)

# Menampilkan beberapa baris pertama dari dataframe yang sudah digabungkan
print(combined_df.head())

"""**Instal Package Sastrawi untuk melakukan Preprocesing Data**"""

pip install Sastrawi

import re
import nltk
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Mengunduh data NLTK yang diperlukan
nltk.download('punkt')
nltk.download('wordnet')

# Inisialisasi stop words bahasa Indonesia dan lemmatizer
factory = StopWordRemoverFactory()
stop_words = set(factory.get_stop_words())
lemmatizer = WordNetLemmatizer()

# Fungsi pra-pemrosesan
def preprocess_text(text):
    # Menghapus kata yang diawali dengan @
    text = re.sub(r'@\w+', '', text)

    # Menghapus karakter khusus dan angka
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\d', ' ', text)

    # Mengubah teks menjadi huruf kecil
    text = text.lower()

    # Tokenisasi teks
    tokens = word_tokenize(text)

    # Menghapus stop words dan melakukan lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    # Menggabungkan token kembali menjadi string teks
    processed_text = ' '.join(tokens)

    return processed_text

# Menerapkan pra-pemrosesan pada kolom 'full_text' dari dataframe gabungan
combined_df['processed_text'] = combined_df['full_text'].apply(preprocess_text)

# Menyimpan dataframe yang sudah diproses ke file CSV baru
combined_df.to_csv('processed_combined_dataset.csv', index=False)

# Menampilkan beberapa baris pertama dari dataframe yang sudah diproses
print(combined_df.head())

"""Karena data crawling belum memiliki sentiment, maka hasil preprocesing perlu ditambahkan sentiment

**Penambahan sentiment pada file preprocesing**
"""

import pandas as pd

df = pd.read_csv('/content/Datasentimen.csv')
df.head()

"""**Klasifikasi dan Evaluasi model menggunakan metode Naive Bayes**"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Membaca data dari CSV yang telah ditambahkan kolom sentimen
df = pd.read_csv('/content/Datasentimen.csv')

# Mengatasi nilai-nilai yang hilang (NaN)
df['processed_text'] = df['processed_text'].fillna('')
df = df.dropna(subset=['sentiment'])

# Pra-pemrosesan teks
df['processed_text'] = df['processed_text'].apply(preprocess_text)

# Memisahkan fitur dan label
X = df['processed_text']
y = df['sentiment']

# Memisahkan data menjadi data latih (80%) dan data uji (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Inisialisasi TF-IDF Vectorizer dengan bigram dan trigram
tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))

# Menyesuaikan dan mengubah data latih menjadi fitur TF-IDF
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

# Mengubah data uji menjadi fitur TF-IDF
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Inisialisasi model Multinomial Naive Bayes dengan smoothing parameter
nb_classifier = MultinomialNB(alpha=0.1)

# Melatih model dengan data latih
nb_classifier.fit(X_train_tfidf, y_train)

# Memprediksi sentimen data uji
y_pred = nb_classifier.predict(X_test_tfidf)

# Mencetak hasil sentimen
results = pd.DataFrame({'Text': X_test, 'Actual Sentiment': y_test, 'Predicted Sentiment': y_pred})
print(results)

# Menghitung akurasi model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAkurasi model: {accuracy * 100:.2f}%")

# Menampilkan laporan klasifikasi
print("\nLaporan klasifikasi:")
print(classification_report(y_test, y_pred))

# Menampilkan matriks kebingungan
print("\nMatriks kebingungan:")
print(confusion_matrix(y_test, y_pred))

"""**Visualisasi Data Model**"""

import matplotlib.pyplot as plt

# Hitung jumlah prediksi positif dan negatif
# Make sure 'positive' and 'negative' match the labels in your 'y_pred' array
positive_count = (y_pred == 'NEGATIF').sum()
negative_count = (y_pred == 'POSITIF').sum()

# Data untuk grafik pie
labels = 'Positif', 'Negatif'
sizes = [positive_count, negative_count]
colors = ['#66b3ff','#ff9999']
explode = (0.1, 0)  # Meledakkan irisan pertama (Positive)

# Membuat grafik pie
plt.figure(figsize=(5, 5))
plt.pie(sizes, explode=explode,  colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Distribusi Sentimen Prediksi')
plt.text(1.3, 0.19, f'Negative: {sizes[1]}',  fontsize=12, color='#ff9999')
plt.text(1.3, 0, f'Positive: {sizes[0]}',  fontsize=12, color='#66b3ff')

plt.show()